{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Taming Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from stop_words import get_stop_words\n",
    "from  email.parser import FeedParser\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Word cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Email corpus extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are not intersted in every details (like the sender/receiver, etc..) our first task is to extract the body of the emails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's read the Emails.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mails = pd.read_csv(\"hillary-clinton-emails/Emails.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mails.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have two options: take the raw text or use the extracted body, but since the content of the raw text seems to be messy (further inspections confirmed this) and as some people already did the work for us, we are going to use the extracted bodies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emailsBody = mails['ExtractedBodyText'].dropna()\n",
    "print(len(emailsBody))\n",
    "emailsBody.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then lower case everything (no important information is lost with this and it may avoid us troubles, plus better uniformity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emailsBody = emailsBody.apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need the data to be represented in a single string, so we first merge all the cells in a single array and then merge that array in a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textArray = emailsBody.values.flatten()\n",
    "textString = ' '.join(textArray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first method is straightforward, generate a cloud directly from the unprocessed, untokenized string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cloud = WordCloud().generate(textString)\n",
    "#cloud = WordCloud(max_font_size=40).generate(rawTextString) limits size of biggest word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(cloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This word cloud shows what we could expect from such emails (state, office, Obama, ...) but also a lot of noisy words that do not give relevant informations, such as 'will', 'said', 'call', etc.. The next steps are intended to improve the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization & stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now tokenize the entire string, Regexp does the big of the tokenization, while \"stop_words\" will get rid of useless words like \"the\", \"and\"... We also remove some useless dominant word we identified in the previous cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tokens = nltk.word_tokenize(rawTextString)\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "allTokens = pd.Series(tokenizer.tokenize(textString))\n",
    "allTokens = allTokens[~allTokens.isin({'will', 'pm', 'said', 'call'})]\n",
    "print(\"Number of tokens: \", len(allTokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "en_stop = get_stop_words('en')\n",
    "stopTokens = allTokens[~allTokens.isin(en_stop)]\n",
    "print(\"Number of tokens after stop word filtering: \", len(stopTokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Remaining tokens: \", round(len(stopTokens) / len(allTokens) * 100), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stop words filtering got rid of 39% of the tokens, which is quite big."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that nltk got rid of unecessary data for us, we'll merge the array back into a string and use wordcloud again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleanString = ' '.join(allTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleanCloud = WordCloud().generate(cleanString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Raw\")\n",
    "plt.imshow(cloud)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print(\"Without stopwords\")\n",
    "plt.imshow(cleanCloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is indeed better, the removal of some dominant words allowed more relevant ones to appear, and it looks pretty nice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next processing step is stemming, for this purpose we are going to use Porter's algorithm, which is already implemented in the stemming python library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemTokens = stopTokens.apply(stemmer.stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = stemTokens #for next part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopStemStr = ' '.join(stemTokens)\n",
    "stemCloud = WordCloud().generate(stopStemStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Without stopwords\")\n",
    "plt.imshow(cleanCloud)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print(\"Without stopwords + stemming\")\n",
    "plt.imshow(stemCloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result here is a bit different, the stemming transformed some of the words, but now we have a more accurate representation of the kind of vocabulary used in these emails. For example, 'work' was not even there in the previous cloud and some irrelevant words lost of their weight like 'also', 'one', 'us', ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 3: Topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary function will assign and ID to each token and do word counting.\n",
    "doc2bow converts a dictionary into a (word_id, word_frequency) tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary = Dictionary([tokens]) #document term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in [tokens]] #bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use the lda function from 5 to 50, printing the topic at each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda = [None]*45\n",
    "for topics in range(5,50):\n",
    "    idx = topics - 5\n",
    "    lda[idx] = LdaModel(corpus, num_topics=topics, id2word = dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(0, len(lda)):\n",
    "    print(lda[idx].print_topics(num_topics=3, num_words=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now that the algorithm generated all the possibilities for each parameter, we observe that a lot of topics contain a single letter (which is due to the previous processing), let's try to redo it but without these noise letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens2 = tokens[~tokens.isin(['s', 't'])]\n",
    "len(tokens) - len(tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = Dictionary([tokens2]) #document term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in [tokens2]] #bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda2 = [None] * 45\n",
    "for topics in range(5,50):\n",
    "    idx = topics - 5\n",
    "    lda2[idx] = LdaModel(corpus, num_topics=topics, id2word = dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(0, len(lda2)):\n",
    "    print(lda2[idx].print_topics(num_topics=3, num_words=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is better, the first thing we notice is that all topics contain the words 'state', and it definitely seems that even with 5 topics we have consistent words in them."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
